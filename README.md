<h2> Pre-trained Languge Model (PLM) Papers </h2>

<ul>

                             

 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(1).pdf" style="text-decoration:none;">Semi-supervised Sequence Learning</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(2).pdf" style="text-decoration:none;">Unsupervised Pretraining for Sequence to Sequence Learning</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(3).pdf" style="text-decoration:none;">Deep contextualized word representations</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(4).pdf" style="text-decoration:none;">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(5).pdf" style="text-decoration:none;">Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(6).pdf" style="text-decoration:none;">Assessing BERT's Syntactic Abilities</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(7).pdf" style="text-decoration:none;">Cross-lingual Language Model Pretraining</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(8).pdf" style="text-decoration:none;"> BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model</a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(9).pdf" style="text-decoration:none;">Distilling Task-Specific Knowledge from BERT into Simple Neural Networks</a></li>
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(10).pdf" style="text-decoration:none;">VideoBERT: A Joint Model for Video and Language Representation Learning</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(11).pdf" style="text-decoration:none;">75 Languages, 1 Model: Parsing Universal Dependencies Universally</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(12).pdf" style="text-decoration:none;">Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(13).pdf" style="text-decoration:none;">ERNIE: Enhanced Representation through Knowledge Integration</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(14).pdf" style="text-decoration:none;">Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(15).pdf" style="text-decoration:none;">Model Compression with Multi-Task Knowledge Distillation for Web-scale Question Answering System</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(16).pdf" style="text-decoration:none;">MASS: Masked Sequence to Sequence Pre-training for Language Generation</a></li>

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(17).pdf" style="text-decoration:none;">Unified Language Model Pre-training for Natural Language Understanding and Generation</a></li>   
  
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(18).pdf" style="text-decoration:none;">What do you learn from context? Probing for sentence structure in contextualized word representations</a></li> 

  
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(19).pdf" style="text-decoration:none;">Are Sixteen Heads Really Better than One?</a></li> 

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(20).pdf" style="text-decoration:none;"> Defending Against Neural Fake News</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(21).pdf" style="text-decoration:none;">Blackbox meets blackbox: Representational Similarity and Stability Analysis of Neural Language Models and Brains</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(22).pdf" style="text-decoration:none;">Open Sesame: Getting Inside BERT’s Linguistic Knowledge</a></li> 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(23).pdf" style="text-decoration:none;">Visualizing and Measuring the Geometry of BERT</a></li> 
 

   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(24).pdf" style="text-decoration:none;">Analyzing the Structure of Attention in a Transformer Language Model</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(25).pdf" style="text-decoration:none;">What Does BERT Look At?
An Analysis of BERT's Attention</a></li>                              
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(26).pdf" style="text-decoration:none;">Learning Video Representations using Contrastive Bidirectional Transformer</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(27).pdf" style="text-decoration:none;">Pre-Training with WholeWord Masking for Chinese BERT</a></li>
   
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(28).pdf" style="text-decoration:none;">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(29).pdf" style="text-decoration:none;">SpanBERT: Improving Pre-training by Representing and Predicting Spans</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(30).pdf" style="text-decoration:none;">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(31).pdf" style="text-decoration:none;">Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment</a></li> 
    <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(32).pdf" style="text-decoration:none;">ERNIE 2.0: A Continual Pre-training Framework for Language Understanding</a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(33).pdf" style="text-decoration:none;">What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(34).pdf" style="text-decoration:none;">ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</a></li> 
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(35).pdf" style="text-decoration:none;">VisualBERT: A Simple and Performant Baseline for Vision and Language</a></li> 

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(36).pdf" style="text-decoration:none;">On Identifiability in Transformers</a></li> 
 
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(37).pdf" style="text-decoration:none;">Fusion of Detected Objects in Text for Visual Question Answering</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(38).pdf" style="text-decoration:none;">Visualizing and Understanding the Effectiveness of BERT</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(39).pdf" style="text-decoration:none;">Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(40).pdf" style="text-decoration:none;">Universal Adversarial Triggers for Attacking and Analyzing NLP</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(41).pdf" style="text-decoration:none;">LXMERT: Learning Cross-Modality Encoder Representations from Transformers</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(42).pdf" style="text-decoration:none;">VL-BERT: Pre-training of Generic Visual-Linguistic Representations</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(43).pdf" style="text-decoration:none;">Well-Read Students Learn Better: On the Importance of Pre-training Compact Models</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(44).pdf" style="text-decoration:none;">Patient Knowledge Distillation for BERT Model Compression</a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(45).pdf" style="text-decoration:none;">Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel</a></li>  
   
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(46).pdf" style="text-decoration:none;">Small and Practical BERT Models for Sequence Labeling</a></li> 
                             
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(47).pdf" style="text-decoration:none;">How Contextual are ContextualizedWord Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(48).pdf" style="text-decoration:none;">Language Models as Knowledge Bases?</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(49).pdf" style="text-decoration:none;">The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(50).pdf" style="text-decoration:none;">Investigating BERT’s Knowledge of Language: Five Analysis Methods with NPIs</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(51).pdf" style="text-decoration:none;">Knowledge Enhanced ContextualWord Representations</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(52).pdf" style="text-decoration:none;">MultiFiT: Efficient Multi-lingual Language Model Fine-tuning</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(53).pdf" style="text-decoration:none;">How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations </a></li>
 
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(54).pdf" style="text-decoration:none;">Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(55).pdf" style="text-decoration:none;">K-BERT: Enabling Language Representation with Knowledge Graph</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(56).pdf" style="text-decoration:none;">Addressing Word-order Divergence in Multilingual Neural Machine Translation for extremely Low Resource Languages </a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(57).pdf" style="text-decoration:none;">Bi-Directional Differentiable Input Reconstruction for Low-Resource Neural Machine Translation </a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(58).pdf" style="text-decoration:none;">Neural Machine Translation with Adequacy-Oriented Learning</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(59).pdf" style="text-decoration:none;">Non-Autoregressive Neural Machine Translation with Enhanced Decoder Input</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(60).pdf" style="text-decoration:none;">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(61).pdf" style="text-decoration:none;">Cross-lingual Language Model Pretraining</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(62).pdf" style="text-decoration:none;">The Evolved Transformer</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(63).pdf" style="text-decoration:none;">Non-Monotonic Sequential Text Generation</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(64).pdf" style="text-decoration:none;">Context-Aware Self-Attention Networks</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(65).pdf" style="text-decoration:none;">Dynamic Layer Aggregation for Neural Machine Translation with Routing-by-Agreement </a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(66).pdf" style="text-decoration:none;">Saliency Learning: Teaching the Model Where to Pay Attention</a></li> 
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(67).pdf" style="text-decoration:none;">Star-Transformer</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(68).pdf" style="text-decoration:none;">Cross-Lingual Alignment of ContextualWord Embeddings, with Applications to Zero-shot Dependency Parsing</a></li> 
 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(69).pdf" style="text-decoration:none;">Improving Robustness of Machine Translation with Synthetic Noise</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(70).pdf" style="text-decoration:none;">Lost in Machine Translation: A Method to Reduce Meaning Loss</a></li> 
  
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(71).pdf" style="text-decoration:none;">Attention is not Explanation</a></li>
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(72).pdf" style="text-decoration:none;">Non-Autoregressive Machine Translation with Auxiliary Regularization</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(73).pdf" style="text-decoration:none;">Reinforcement Learning based Curriculum Optimization for Neural Machine Translation</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(74).pdf" style="text-decoration:none;">Non-Parametric Adaptation for Neural Machine Translation</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(75).pdf" style="text-decoration:none;">Massively Multilingual Neural Machine Translation</a></li>                        
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(76).pdf" style="text-decoration:none;">Context-Aware Cross-Lingual Mapping</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(77).pdf" style="text-decoration:none;">On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(78).pdf" style="text-decoration:none;">compare-mt: A Tool for Holistic Comparison of Language Generation Systems</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(79).pdf" style="text-decoration:none;">Probing the Need for Visual Context in Multimodal Machine Translation</a></li>


 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(80).pdf" style="text-decoration:none;">Selective Attention for Context-aware Neural Machine Translation</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(81).pdf" style="text-decoration:none;">Pre-trained Language Model Representations for Language Generation</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(82).pdf" style="text-decoration:none;">Competence-based Curriculum Learning for Neural Machine Translation</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(83).pdf" style="text-decoration:none;">Aligning Vector-spaces with Noisy Supervised Lexicons</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(84).pdf" style="text-decoration:none;">Train, Sort, Explain: Learning to Diagnose Translation Models</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(85).pdf" style="text-decoration:none;">Lost in Interpretation:
Predicting Untranslated Terminology in Simultaneous Interpretation</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(86).pdf" style="text-decoration:none;">Learning to Stop in Structured Prediction for Neural Machine Translation</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(87).pdf" style="text-decoration:none;">Extract and Edit: An Alternative to Back-Translation for Unsupervised Neural Machine Translation</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(88).pdf" style="text-decoration:none;">Consistency by Agreement in Zero-shot Neural Machine Translation</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(89).pdf" style="text-decoration:none;">Density Matching for Bilingual Word Embedding</a></li>
  
  
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(90).pdf" style="text-decoration:none;">ReWE: RegressingWord Embeddings for Regularization of Neural Machine Translation Systems</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(91).pdf" style="text-decoration:none;">Modeling Recurrence for Transformer</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(92).pdf" style="text-decoration:none;">Information Aggregation for Multi-Head Attention with Routing-by-Agreement</a></li>
  </ul>
