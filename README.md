<h2> Pre-trained Languge Model (PLM) Papers </h2>

<ul>

                             

 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(1).pdf" style="text-decoration:none;">Semi-supervised Sequence Learning</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(2).pdf" style="text-decoration:none;">Unsupervised Pretraining for Sequence to Sequence Learning</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(3).pdf" style="text-decoration:none;">Deep contextualized word representations</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(4).pdf" style="text-decoration:none;">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(5).pdf" style="text-decoration:none;">Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(6).pdf" style="text-decoration:none;">Assessing BERT's Syntactic Abilities</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(7).pdf" style="text-decoration:none;">Cross-lingual Language Model Pretraining</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(8).pdf" style="text-decoration:none;"> BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model</a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(9).pdf" style="text-decoration:none;">Distilling Task-Specific Knowledge from BERT into Simple Neural Networks</a></li>
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(10).pdf" style="text-decoration:none;">VideoBERT: A Joint Model for Video and Language Representation Learning</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(11).pdf" style="text-decoration:none;">75 Languages, 1 Model: Parsing Universal Dependencies Universally</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(12).pdf" style="text-decoration:none;">Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(13).pdf" style="text-decoration:none;">ERNIE: Enhanced Representation through Knowledge Integration</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(14).pdf" style="text-decoration:none;">Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(15).pdf" style="text-decoration:none;">Model Compression with Multi-Task Knowledge Distillation for Web-scale Question Answering System</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(16).pdf" style="text-decoration:none;">MASS: Masked Sequence to Sequence Pre-training for Language Generation</a></li>

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(17).pdf" style="text-decoration:none;">Unified Language Model Pre-training for Natural Language Understanding and Generation</a></li>   
  
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(18).pdf" style="text-decoration:none;">What do you learn from context? Probing for sentence structure in contextualized word representations</a></li> 

  
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(19).pdf" style="text-decoration:none;">Are Sixteen Heads Really Better than One?</a></li> 

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(20).pdf" style="text-decoration:none;"> Defending Against Neural Fake News</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(21).pdf" style="text-decoration:none;">Blackbox meets blackbox: Representational Similarity and Stability Analysis of Neural Language Models and Brains</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(22).pdf" style="text-decoration:none;">Open Sesame: Getting Inside BERT’s Linguistic Knowledge</a></li> 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(23).pdf" style="text-decoration:none;">Visualizing and Measuring the Geometry of BERT</a></li> 
 

   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(24).pdf" style="text-decoration:none;">Analyzing the Structure of Attention in a Transformer Language Model</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(25).pdf" style="text-decoration:none;">What Does BERT Look At?
An Analysis of BERT's Attention</a></li>                              
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(26).pdf" style="text-decoration:none;">Learning Video Representations using Contrastive Bidirectional Transformer</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(27).pdf" style="text-decoration:none;">Pre-Training with WholeWord Masking for Chinese BERT</a></li>
   
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(28).pdf" style="text-decoration:none;">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(29).pdf" style="text-decoration:none;">SpanBERT: Improving Pre-training by Representing and Predicting Spans</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(30).pdf" style="text-decoration:none;">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(31).pdf" style="text-decoration:none;">Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment</a></li> 
    <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(32).pdf" style="text-decoration:none;">ERNIE 2.0: A Continual Pre-training Framework for Language Understanding</a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(33).pdf" style="text-decoration:none;">What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(34).pdf" style="text-decoration:none;">ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</a></li> 
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(35).pdf" style="text-decoration:none;">VisualBERT: A Simple and Performant Baseline for Vision and Language</a></li> 

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(36).pdf" style="text-decoration:none;">On Identifiability in Transformers</a></li> 
 
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(37).pdf" style="text-decoration:none;">Fusion of Detected Objects in Text for Visual Question Answering</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(38).pdf" style="text-decoration:none;">Visualizing and Understanding the Effectiveness of BERT</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(39).pdf" style="text-decoration:none;">Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(40).pdf" style="text-decoration:none;">Universal Adversarial Triggers for Attacking and Analyzing NLP</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(41).pdf" style="text-decoration:none;">LXMERT: Learning Cross-Modality Encoder Representations from Transformers</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(42).pdf" style="text-decoration:none;">VL-BERT: Pre-training of Generic Visual-Linguistic Representations</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(43).pdf" style="text-decoration:none;">Well-Read Students Learn Better: On the Importance of Pre-training Compact Models</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(44).pdf" style="text-decoration:none;">Patient Knowledge Distillation for BERT Model Compression</a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(45).pdf" style="text-decoration:none;">Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel</a></li>  
   
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(46).pdf" style="text-decoration:none;">Small and Practical BERT Models for Sequence Labeling</a></li> 
                             
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(47).pdf" style="text-decoration:none;">How Contextual are ContextualizedWord Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(48).pdf" style="text-decoration:none;">Language Models as Knowledge Bases?</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(49).pdf" style="text-decoration:none;">The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(50).pdf" style="text-decoration:none;">Investigating BERT’s Knowledge of Language: Five Analysis Methods with NPIs</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(51).pdf" style="text-decoration:none;">Knowledge Enhanced ContextualWord Representations</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(52).pdf" style="text-decoration:none;">MultiFiT: Efficient Multi-lingual Language Model Fine-tuning</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(53).pdf" style="text-decoration:none;">How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations </a></li>
 
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(54).pdf" style="text-decoration:none;">Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(55).pdf" style="text-decoration:none;">K-BERT: Enabling Language Representation with Knowledge Graph</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(56).pdf" style="text-decoration:none;">Do NLP Models Know Numbers? Probing Numeracy in Embeddings </a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(57).pdf" style="text-decoration:none;">TinyBERT: Distilling BERT for Natural Language Understanding </a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(58).pdf" style="text-decoration:none;">Extreme Language Model Compression with Optimal Subwords and Shared Projections</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(59).pdf" style="text-decoration:none;">UNITER: UNiversal Image-TExt Representation Learning</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(60).pdf" style="text-decoration:none;">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(61).pdf" style="text-decoration:none;">Transformers: State-of-the-Art Natural Language Processing</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(62).pdf" style="text-decoration:none;">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(63).pdf" style="text-decoration:none;">On the Cross-lingual Transferability of Monolingual Representations</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(64).pdf" style="text-decoration:none;">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(65).pdf" style="text-decoration:none;">How Language-Neutral is Multilingual BERT? </a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(66).pdf" style="text-decoration:none;">ExpBERT: Representation Engineering with Natural Language Explanations</a></li> 
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(67).pdf" style="text-decoration:none;">SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(68).pdf" style="text-decoration:none;">DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference</a></li> 
 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(69).pdf" style="text-decoration:none;">Do You Have the Right Scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(70).pdf" style="text-decoration:none;">BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance</a></li> 
  
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(71).pdf" style="text-decoration:none;">Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT</a></li>
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(72).pdf" style="text-decoration:none;">TAPAS: Weakly Supervised Table Parsing via Pre-training</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(73).pdf" style="text-decoration:none;">Finding Universal Grammatical Relations in Multilingual BERT</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(74).pdf" style="text-decoration:none;">Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(75).pdf" style="text-decoration:none;">Don't Stop Pretraining: Adapt Language Models to Domains and Tasks</a></li>                        
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(76).pdf" style="text-decoration:none;">TABERT: Pretraining for Joint Understanding of Textual and Tabular Data</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(77).pdf" style="text-decoration:none;">A Mutual Information Maximization Perspective of Language Representation Learning</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(78).pdf" style="text-decoration:none;">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(79).pdf" style="text-decoration:none;">Cross-Lingual Ability of Multilingual BERT: An Empirical Study</a></li>


 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(80).pdf" style="text-decoration:none;">BERT is Not an Interlingua and the Bias of Tokenization</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(81).pdf" style="text-decoration:none;">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(82).pdf" style="text-decoration:none;">FreeLB: Enhanced Adversarial Training for Natural Language Understanding</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(83).pdf" style="text-decoration:none;">context2vec: Learning Generic Context Embedding with Bidirectional LSTM</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(84).pdf" style="text-decoration:none;">Language Models are Unsupervised Multitask Learners</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(85).pdf" style="text-decoration:none;">Improving Language Understanding by Generative Pre-Training</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(86).pdf" style="text-decoration:none;">Multilingual Alignment of Contextual Word Representations</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(87).pdf" style="text-decoration:none;">A Structural Probe for Finding Syntax in Word Representations</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(88).pdf" style="text-decoration:none;">Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(89).pdf" style="text-decoration:none;">Reducing Transformer Depth on Demand with Structured Dropout</a></li>
  
  
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(90).pdf" style="text-decoration:none;">StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(91).pdf" style="text-decoration:none;">Thieves on Sesame Street! Model Extraction of BERT-based APIs</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Languge-Model-Papers/blob/master/ptl(92).pdf" style="text-decoration:none;">To Tune or Not to Tune?
Adapting Pretrained Representations to Diverse Tasks</a></li>
  </ul>
